<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Human-Guided AMM Framework</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
        }
        header {
            background: #333;
            color: white;
            padding: 10px 20px;
        }
        header h1 {
            margin: 0;
            text-align: center;
        }
        section {
            padding: 20px;
        }
        h2 {
            color: #333;
            text-align: center;
        }
        ul {
            list-style-type: disc;
            margin: 10px 0 20px 20px;
        }
        footer {
            background: #333;
            color: white;
            text-align: center;
            padding: 10px 0;
        }
    </style>
</head>
<body>
    <header>
        <h1 style="text-align:center">Human-Guided Autonomous Mobile Manipulator Case Study: LoCoBot WX200</h1>
    </header>

    <iframe id='dashboard-frame' width='100%' height='500px' allowfullscreen src="https://www.youtube.com/embed/kPX0rDkTCa0">
    </iframe>

    <section>
        <h2>Context and Objectives</h2>
        <p>The project explores the implementation of a human-guided framework for autonomous robots capable of real-world interaction with humans. The chosen platform, the LoCoBot WX200, combines mobility and manipulation capabilities for diverse applications, including industrial automation and service robotics.</p>
        <p>The primary goals of this project were:</p>
        <ul>
            <li><strong>Navigation:</strong> Incorporating features like human-following, obstacle detection and avoidance, and precise pose estimation using external cameras.</li>
            <li><strong>Interaction:</strong> Enabling intuitive human-robot collaboration using gesture recognition, object handling, and seamless communication with human operators.</li>
        </ul>
    </section>

    <section>
        <h2>Developed Framework</h2>
        <p>The project’s framework is modular and scalable, allowing easy adaptation to different applications. The main components include:</p>
        <ul>
            <li><strong>Navigation Module:</strong> A customized implementation of the Nav2 stack to provide robust navigation capabilities. This module handles tasks such as human-following and real-time obstacle avoidance.</li>
            <li><strong>Interaction Module:</strong> Built using MoveIt 2, this module controls the robotic arm for performing various tasks, including object manipulation and interaction with humans.</li>
            <li><strong>Gesture Recognition:</strong> A MediaPipe-based neural network that recognizes up to seven predefined hand gestures. It provides the primary input method for human commands, ensuring intuitive control.</li>
            <li><strong>Localization Module:</strong> Utilizes Apriltags and Kinect cameras for accurate pose estimation of both the robot and the human in the environment. This module ensures smooth navigation and reliable human-following.</li>
        </ul>
    </section>

    <section>
        <h2>Validation and Results</h2>
        <p>The framework was validated through a series of tests inspired by the ISO 18646-2 standard, focusing on the robot’s performance in real-world scenarios. The validation process assessed:</p>
        <ul>
            <li><strong>Navigation:</strong> The robot demonstrated efficient human-following and obstacle avoidance, even in dynamic environments with unmapped obstacles.</li>
            <li><strong>Gesture Recognition:</strong> The MediaPipe model accurately detected hand gestures, although minor delays were noted that could affect user experience.</li>
            <li><strong>Interaction:</strong> The robotic arm successfully executed all planned movements, ensuring reliable performance during interaction tasks.</li>
            <li><strong>Localization:</strong> Accurate pose estimation was achieved, aided by careful calibration of the Kinect cameras and Apriltags.</li>
        </ul>
        <p><strong>Limitations:</strong> Challenges included the robot occasionally classifying humans as obstacles during close-range following and the use of a fixed arm pose for interaction, which limited flexibility.</p>
    </section>

    <section>
        <h2>Conclusions</h2>
        <p>The project achieved its primary objectives, demonstrating the feasibility and potential of the proposed framework. The results confirm the effectiveness of integrating Nav2 and MoveIt 2 stacks within a modular architecture for autonomous navigation and human-robot interaction.</p>
        <p>Potential improvements for future work include:</p>
        <ul>
            <li>Integrating a LIDAR sensor to improve navigation safety and reliability.</li>
            <li>Exploring alternative input methods, such as voice recognition, to enhance user experience and eliminate proximity constraints.</li>
            <li>Developing dynamic pose generation for the robotic arm to increase operational flexibility.</li>
        </ul>
        <p>Overall, the framework provides a robust foundation for further research and development in human-robot interaction and autonomous systems.</p>
    </section>

    <section>
        <h2>Thesis </h2>
        <p>The thesis is available at the following link: <a href="https://hdl.handle.net/20.500.12608/77775" target="_blank">https://hdl.handle.net/20.500.12608/77775</a></p>
    </section>

    <footer>
        <p>Created by Alessio Lovato, University of Padua - 2023/2024</p>
    </footer>
</body>
</html>